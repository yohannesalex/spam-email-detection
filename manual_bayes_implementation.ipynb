{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Spam Email Classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This project uses Bayesian classification to detect spam emails based on the SMS Spam Collection Dataset from UCI. By leveraging probabilities derived from the data, the model classifies messages as either \"spam\" or \"ham\" (not spam).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset used is a text file with two columns:\n",
    "- label: Indicates whether the message is \"spam\" or \"ham.\"\n",
    "- message: The content of the SMS message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- pandas (pd): For handling and analyzing structured data (dataframes).\n",
    "- numpy (np): For numerical operations and array manipulations.\n",
    "- re: For regular expression operations (text processing).\n",
    "- csv: For reading and writing CSV files.\n",
    "- nltk: For natural language processing tasks.\n",
    "- stopwords: To remove common words that don't add much meaning (e.g., \"the\", \"and\").\n",
    "- word_tokenize: To split text into individual words (tokenization).\n",
    "- WordNetLemmatizer: To reduce words to their base forms (lemmatization).\n",
    "- log (from math): For calculating logarithms (used in probability calculations).\n",
    "- train_test_split (from sklearn.model_selection): For splitting data into training and testing sets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset into a usable format (CSV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully converted to ./data/data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_file_path = './data/SMSSpamCollection.txt'\n",
    "output_file_path = './data/data.csv'\n",
    "\n",
    "with open(input_file_path, 'r') as txt_file:\n",
    "    lines = txt_file.readlines()\n",
    "\n",
    "processed_data = []\n",
    "for line in lines:\n",
    "    label, message = line.strip().split('\\t', 1)  # Only split at the first tab\n",
    "    processed_data.append([label, message])\n",
    "\n",
    "with open(output_file_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['label', 'message'])\n",
    "    writer.writerows(processed_data)\n",
    "\n",
    "print(f\"Data has been successfully converted to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "### 1. Binary Labeling:\n",
    "Map the label column:\n",
    "- \"spam\" ‚Üí 1\n",
    "- \"ham\" ‚Üí 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  binary_label\n",
       "0   ham  Go until jurong point, crazy.. Available only ...             0\n",
       "1   ham                      Ok lar... Joking wif u oni...             0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...             1\n",
       "3   ham  U dun say so early hor... U c already then say...             0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...             0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/data.csv\", encoding=\"latin-1\")\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "df['binary_label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download the necessary NLTK datasets for stopwords, tokenization, and word lemmatization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Tokenization and Data cleaning:\n",
    "- Split messages into individual words for word-frequency analysis using the NLTK library.\n",
    "- Remove unnecessary characters, stop words, and special symbols from the text.\n",
    "- convert each word to their dictionary format (This is for not to analyse different forms of the same word many times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed'] = df['message'].apply(preprocess)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities\n",
    "### Bayes' Theorem\n",
    "- The spam detection system uses Bayes' theorem:\n",
    "- ùëÉ(spam‚à£words)=ùëÉ(words‚à£spam)‚ãÖùëÉ(spam)/ùëÉ(words)\n",
    "\n",
    "‚Äã\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, Prior Probability:\n",
    "Calculated the prior probability of spam and ham message\n",
    "- P(spam): Probability of a message being spam.\n",
    "- P(ham): Probability of a message being ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam): 0.13, P(ham): 0.87\n"
     ]
    }
   ],
   "source": [
    "# Calculate priors\n",
    "total_messages = len(df)\n",
    "spam_messages = df['binary_label'].sum()\n",
    "ham_messages = total_messages - spam_messages\n",
    "\n",
    "P_spam = spam_messages / total_messages\n",
    "P_ham = ham_messages / total_messages\n",
    "\n",
    "print(f\"P(spam): {P_spam:.2f}, P(ham): {P_ham:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, likelihood \n",
    "- P(word‚à£spam): Probability of a specific word given the message is spam.\n",
    "- P(word‚à£ham): Probability of a specific word given the message is ham.\n",
    "    - Split Messages: Separates words into spam_words and ham_words based on the binary_label (1 for spam, 0 for ham).\n",
    "    - Frequency Distribution: Counts occurrences of each word in spam and ham messages (spam_word_count, ham_word_count).\n",
    "    - Total Word Counts: Calculates total words in spam and ham messages.\n",
    "    - Unique Words: Finds unique words across both spam and ham, determining vocabulary_size.\n",
    "    - Likelihood Calculation: Uses Laplace smoothing to compute \n",
    "- Laplace smoothing\n",
    "    - is a technique used to handle zero probabilities in probabilistic models like Naive Bayes. It adds a small constant (typically 1) to the count of each word to ensure no probability is zero, even for words not seen in the training data.\n",
    "    The formula for Laplace smoothing is:\n",
    "        P(word‚à£label)=(count(word)+1) / (total¬†words¬†in¬†label+vocabulary¬†size)\n",
    "    - This approach prevents the model from assigning a probability of zero to unseen words, making the model more robust and generalizable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('free'|spam): 0.0115\n",
      "P('free'|ham): 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Split into spam and ham messages\n",
    "spam_words = []\n",
    "ham_words = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['binary_label'] == 1:\n",
    "        spam_words.extend(row['processed'])\n",
    "    else:\n",
    "        ham_words.extend(row['processed'])\n",
    "\n",
    "# Create frequency distributions\n",
    "spam_word_count = pd.Series(spam_words).value_counts()\n",
    "ham_word_count = pd.Series(ham_words).value_counts()\n",
    "# Total word counts\n",
    "total_spam_words = sum(spam_word_count)\n",
    "total_ham_words = sum(ham_word_count)\n",
    "\n",
    "# Unique words across spam and ham\n",
    "unique_words = set(spam_word_count.index).union(set(ham_word_count.index))\n",
    "vocabulary_size = len(unique_words)\n",
    "\n",
    "# Likelihood calculation with Laplace smoothing\n",
    "def calculate_likelihood(word, word_count, total_words):\n",
    "    return (word_count.get(word, 0) + 1) / (total_words + vocabulary_size)\n",
    "\n",
    "# Example for the word \"free\"\n",
    "print(f\"P('free'|spam): {calculate_likelihood('free', spam_word_count, total_spam_words):.4f}\")\n",
    "print(f\"P('free'|ham): {calculate_likelihood('free', ham_word_count, total_ham_words):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior Distribution\n",
    "after preprocessing \n",
    "- For each word in the email, the log likelihoods of the word given spam and ham are added to spam_prob and ham_prob.\n",
    "- The email is classified as spam (1) if spam_prob is greater than ham_prob, otherwise it's classified as ham (0).\n",
    "#### why do i use logarithim?\n",
    "1.  Preventing Underflow:\n",
    "Probabilities are often very small, especially when multiplying many of them together (as in Naive Bayes).\n",
    "Multiplying many small probabilities can lead to underflow, where the result is too small for the computer to represent accurately.\n",
    "Logarithms convert multiplication of probabilities into addition, which avoids this issue.\n",
    "2. Simplifies Multiplication:\n",
    "Instead of multiplying probabilities, we add their logarithms:\n",
    "log(a√ób)=log(a)+log(b)\n",
    "This makes the computations more stable and easier to handle, especially for large datasets.\n",
    "3. Better Numerical Stability:\n",
    "Logarithmic operations are less sensitive to numerical precision issues compared to directly working with very small probabilities.\n",
    "4. Efficient Calculation:\n",
    "Adding numbers (logarithms of probabilities) is computationally more efficient than multiplying a long chain of small numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(email):\n",
    "    words = preprocess(email)\n",
    "    spam_prob = log(P_spam)\n",
    "    ham_prob = log(P_ham)\n",
    "    \n",
    "    for word in words:\n",
    "        spam_prob += log(calculate_likelihood(word, spam_word_count, total_spam_words))\n",
    "        ham_prob += log(calculate_likelihood(word, ham_word_count, total_ham_words))\n",
    "    \n",
    "    return 1 if spam_prob >= ham_prob else 0  # 1 for spam, 0 for ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- splits the dataset into 80% training and 20% testing data.\n",
    "- The predict function is applied to each message in the test_data to generate predictions.\n",
    "- Compares the predicted labels with the actual binary_label in test_data.\n",
    "- Calculates the proportion of correct predictions and prints the accuracy percentage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.83%\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate the classifier\n",
    "test_data['prediction'] = test_data['message'].apply(predict)\n",
    "accuracy = (test_data['prediction'] == test_data['binary_label']).mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As We can see the model has almost 99% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classify an email as spam or not based on the computed posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Ham\n",
      "Ham\n",
      "Ham\n",
      "Ham\n"
     ]
    }
   ],
   "source": [
    "messages = [\"Congratulations! You've won a $1,000 gift card. Click here to claim your prize now!\",\n",
    "\"URGENT: Your account has been compromised. Please verify your details immediately by clicking this link.\",\n",
    "\"Get a loan with no credit check! Apply now and receive up to $10,000 instantly!\",\n",
    "\"Hey, are we still meeting for coffee tomorrow at 10 AM?\",\n",
    "\"Don't forget about the team meeting on Monday. Let me know if you need anything prepared.\",\n",
    "\"Can you send me the notes from yesterday's class? I missed a few key points.\",\n",
    "\"Happy Birthday! Hope you have a fantastic day filled with joy and laughter.\"\n",
    "]\n",
    "for message in messages:\n",
    "    result = predict(message)\n",
    "    if result == 1:\n",
    "        print(\"Spam\")\n",
    "    else:\n",
    "        print(\"Ham\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
